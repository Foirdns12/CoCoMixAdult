from lib.kde import compute_kde
import numpy as np
from data.immo import load_data
from sklearn.model_selection import train_test_split
import scipy.stats as sc
from statsmodels.nonparametric.kernel_density import KDEMultivariate

# %matplotlib inline
import numpy as np
from scipy import stats
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.distributions.mixture_rvs import mixture_rvs

samples, targets, features = load_data()

var_types = np.array(["u", "u", "c", "c", "c", "u", "c", "u", "u", "u", "c", "u", "u",
                      "u"])  # einzigen beiden Punkte, die für ein anderes Dataset/Varibalenauswahl geändert werden müssen
bw = [0.8, 0.01, 0.01, 0.023, 0.11, 0.4, 0.01, 0.0128, 0.01, 0.01, 0.01, 0.152, 0.01, 0.01, 0.01]

# obj_regio_3 sollte sehr starke beschränkungen haben, da es nicht in jedem kreis so eine stadt gibt, ebenso geo_krs mit regio_1
assert len(var_types) == len(features)

VALUES = {

}
MAD = {

}

for i in range(0, len(var_types)):
    if var_types[i] != "c":
        val = np.unique(samples[:, i])
        name = features[i]
        VALUES[name] = val

categorical_values = [VALUES[feature] for idx, feature in enumerate(features)  # hier nicht notwendig da kein ? oder?
                      if var_types[idx] != "c"]

samplestrain, samplestest, targetstrain, targetstest = train_test_split(samples, targets, train_size=.7)

# print(len(samplestrain))

print(samples[1])
#print(np.median(samples[:,11]))
#samples[1][3] = 640 #

#['Rheinland_Pfalz' 'floor_heating' 'y' 470.0 2020.0 2.0 'n' 152.0
 #'Neustadt_an_der_WeinstraÃŸe' 'first_time_use' 'sophisticated' 5.0 'n'
 #'single_family_house' 'n']
 #660 -1.03451213e-01
 #640 -1.05642058e-01
 #680 -1.01260367e-01

#1966 6.26595082e-02
#1969 1.18076911e-01
#1963 7.24209200e-03

#3 1.22082784e-01
#2 -2.71809466e-01
#4 5.15973770e-01

#164 -1.34111716e-01
#159 -1.46888651e-01
#169 -1.21334779e-01

#6 -1.93277734e-01
#5 -3.44643428e-01
#7 -4.19117451e-02



#print(abs(-1.93277734e-01--3.44643428e-01))
#print(abs(-1.93277734e-01--4.19117451e-02))







# orig:
# [-1.24264245e-01  1.06015980e+00 -2.71809466e-01 -1.64776357e-01
# -3.44643428e-01  1.00000000e+01  4.00000000e+00  1.00000000e+00
# 0.00000000e+00  2.50000000e+02  0.00000000e+00  4.00000000e+00c
# 0.00000000e+00  9.00000000e+00  0.00000000e+00]

#1:
#[-1.75639556e-01  1.06015980e+00 -2.71809466e-01 -1.64776357e-01
 #-3.44643428e-01  1.00000000e+01  4.00000000e+00  1.00000000e+00
 # 0.00000000e+00  2.50000000e+02  0.00000000e+00  4.00000000e+00
 # 0.00000000e+00  9.00000000e+00  0.00000000e+00]

 #99999:
#[ 1.07644750e+01  1.06015980e+00 -2.71809466e-01 -1.64776357e-01
 #-3.44643428e-01  1.00000000e+01  4.00000000e+00  1.00000000e+00
 # 0.00000000e+00  2.50000000e+02  0.00000000e+00  4.00000000e+00
 # 0.00000000e+00  9.00000000e+00  0.00000000e+00]


pdf = compute_kde(samples, var_types, categorical_values, 'normal reference') #'normal reference' kann für kde unter 0 sorgen

print("Wkt")
print(pdf(samples[1]))
#samples[1][0]='Bayern'
p#rint(pdf(samples[1]))

#['Rheinland_Pfalz' 'floor_heating' 'y' 470.0 2020.0 2.0 'n' 152.0
# 'Neustadt_an_der_WeinstraÃŸe' 'first_time_use' 'sophisticated' 5.0 'n'
# 'single_family_house' 'n'] liefert negative kde

#print(features)

# t=KDEMultivariate(samplestrain[:,4], ['c'], bw=[0.3])
# print(t)

# _,bw2=compute_kde(samplestrain,var_types,categorical_values,'cv_ml')
# print(features)
# print(bw2)
# _,bw2=compute_kde(samplestrain,var_types,categorical_values,'cv_ls')
# print(features)
# print(bw2)


fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111)
obs_dist = samplestrain[:, 11]
# Scatter plot of data samples and histogram
# ax.scatter(obs_dist, np.abs(np.random.randn(obs_dist.size)),
#            zorder=15, color='red', marker='x', alpha=0.5, label='Samples')
lines = ax.hist(obs_dist, bins=200, edgecolor='k', label='Histogram')

ax.legend(loc='best')
ax.grid(True, zorder=-5)

kde = sm.nonparametric.KDEUnivariate(obs_dist)
kde.fit()

fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111)

# Plot the histrogram
# ax.hist(obs_dist, bins=200, density=True, label='Histogram from samples',
#       zorder=5, edgecolor='k', alpha=0.5)

# Plot the KDE as fitted using the default arguments
ax.plot(kde.support, kde.density, lw=3, label='KDE from samples', zorder=10)

# Plot the samples
# ax.scatter(obs_dist, np.abs(np.random.randn(obs_dist.size))/40,
#          marker='x', color='red', zorder=20, label='Samples', alpha=0.5)

ax.legend(loc='best')
ax.grid(True, zorder=-5)

fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(111)

# Plot the histrogram
ax.hist(obs_dist, bins=200, label='Histogram from samples',
        zorder=5, edgecolor='k', density=True, alpha=0.5)

# Plot the KDE for various bandwidths
for bandwidth in [20]:
    kde.fit(bw=bandwidth)  # Estimate the densities
    ax.plot(kde.support, kde.density, '--', lw=2, color='k', zorder=10,
            label='KDE from samples, bw = {}'.format(round(bandwidth, 5)))

# Plot the samples
# ax.scatter(obs_dist, np.abs(np.random.randn(obs_dist.size))/50,
#          marker='x', color='red', zorder=20, label='Data samples', alpha=0.5)

ax.legend(loc='best')
ax.set_xlim([0, 20])
ax.grid(True, zorder=-5)

plt.show()
